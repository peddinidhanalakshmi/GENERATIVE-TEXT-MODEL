# TASK 4 - GENERATIVE TEXT MODEL
# Using LSTM to generate text based on user input

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import random

# -------------------------------
# Step 1: Sample Training Data
# -------------------------------
data = """Artificial intelligence is transforming the world.
Machine learning allows systems to learn from data.
Natural language processing enables computers to understand human language.
Deep learning is a subset of machine learning using neural networks.
Technology continues to evolve rapidly in the digital era.
Data science helps in extracting knowledge from data.
Robotics combines hardware and AI to perform tasks autonomously."""

# -------------------------------
# Step 2: Tokenization
# -------------------------------
tokenizer = Tokenizer()
tokenizer.fit_on_texts([data])
total_words = len(tokenizer.word_index) + 1

# Convert text into input sequences
input_sequences = []
for line in data.split('.'):
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(2, len(token_list)):
        n_gram_sequence = token_list[:i]
        input_sequences.append(n_gram_sequence)

# Pad sequences to the same length
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# Split predictors and label
X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = tf.keras.utils.to_categorical(y, num_classes=total_words)

# -------------------------------
# Step 3: Build the LSTM Model
# -------------------------------
model = Sequential()
model.add(Embedding(total_words, 64, input_length=max_sequence_len - 1))
model.add(LSTM(100))
model.add(Dense(total_words, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# -------------------------------
# Step 4: Train the Model
# -------------------------------
history = model.fit(X, y, epochs=5, verbose=1)

# -------------------------------
# Step 5: Text Generation Function
# -------------------------------
def generate_text(seed_text, next_words=20):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]
        
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                seed_text += " " + word
                break
    return seed_text

# -------------------------------
# Step 6: Generate Example Output
# -------------------------------
print("Generated Text Samples:\n")

topics = ["artificial intelligence", "machine learning", "data science"]
for topic in topics:
    print(f"Prompt: {topic}")
    print(generate_text(topic, next_words=15))
    print("-" * 60)